{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from collections.abc import Iterable\n",
    "\n",
    "batchSize = 32 #Batch size of training set\n",
    "\n",
    "#import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(model, loss_function, optimizer, numEpochs, dataloader, numInputs):\n",
    "    #Function always assumes that intens column is first column\n",
    "    #In the future, perhaps move the intensity processing to a different function?\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "            # Get and prepare input\n",
    "            totalColumns = data.size()[1]\n",
    "            numOutputs = totalColumns - numInputs\n",
    "\n",
    "            preProcessedInputs = data[:, 0:numInputs] #This line doesn't really do anything, delete later?\n",
    "            targets = data[:, numInputs:(numInputs+numOutputs)]\n",
    "\n",
    "            #Process intensity by putting it on a log scale\n",
    "            intens = data[:, 0:1]\n",
    "            intens = np.log(intens)\n",
    "            inputs = torch.cat((intens, data[:,1:numInputs]), axis = 1)\n",
    "\n",
    "            #Process targets by putting them on a log scale\n",
    "            targets = np.log(targets)\n",
    "\n",
    "            #print(type(inputs))\n",
    "\n",
    "            #Comment the next two lines out if not using GPU\n",
    "            inputs = inputs.to('cuda')\n",
    "            targets = targets.to('cuda')\n",
    "\n",
    "            #Normalize inputs\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            targets = targets.reshape((targets.shape[0], numOutputs))\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            inputs = inputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #The following two lines are for debugging only\n",
    "    #         if i % 10 == 0:\n",
    "    #             print(\"Targets:\", targets[0:2])\n",
    "    #             print(\"Outputs:\", outputs[0:2])\n",
    "    #             print()\n",
    "    #             print()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 10 == 0:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                     (i + 1, current_loss / 500))\n",
    "                current_loss = 0.0\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See if I can import all of the data from a file\n",
    "\n",
    "points = 20000\n",
    "\n",
    "# filename = 'Dataset/Data_Fuchs_v_2.7_Wright_Pat_Narrow_Range_energy_limit_0.01_deviation_0.1_lambda_um_0.8_points_' \\\n",
    "#                 + str(points) + '_seed_0.h5'\n",
    "\n",
    "filename = 'Dataset/Data_Fuchs_v_2.2_Wright_Pat_Narrow_Range_lambda_um_0.8_points_10000_seed_0.h5'\n",
    "\n",
    "h5File = h5py.File(filename, 'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.538453590906543e+18, 40.0, 4.225643132833221, 2.0, 0.010835492564865762, 1.4856161129848022e-08, 0.001689811439248253)\n",
      "<class 'tuple'>\n",
      "(10000,)\n",
      "(1, 10000, 5)\n",
      "(10000, 5)\n",
      "Average Energy: [0.00168981 0.00903983 0.00080004 ... 0.00163991 0.0001079  0.0028896 ]\n",
      "Total Energy: [1.48561611e-08 2.41391952e-07 5.61703127e-09 ... 2.29250478e-08\n",
      " 8.74046945e-11 5.95510623e-08]\n",
      "[[3.53845359e+18 4.22564313e+00 1.08354926e-02 1.48561611e-08\n",
      "  1.68981144e-03]\n",
      " [5.19026302e+18 8.90987569e-01 6.24126123e-02 2.41391952e-07\n",
      "  9.03983370e-03]\n",
      " [4.00648366e+18 9.27135540e+00 5.00937097e-03 5.61703127e-09\n",
      "  8.00040532e-04]\n",
      " ...\n",
      " [5.73362811e+18 8.60258979e+00 1.03812396e-02 2.29250478e-08\n",
      "  1.63990717e-03]\n",
      " [1.05630036e+18 7.47518077e+00 6.65338584e-04 8.74046945e-11\n",
      "  1.07899169e-04]\n",
      " [6.50991143e+18 6.57047545e+00 1.85702376e-02 5.95510623e-08\n",
      "  2.88959944e-03]]\n",
      "[[3.53845359e+18 4.22564313e+00 1.08354926e-02 1.48561611e-08\n",
      "  1.68981144e-03]\n",
      " [5.19026302e+18 8.90987569e-01 6.24126123e-02 2.41391952e-07\n",
      "  9.03983370e-03]\n",
      " [4.00648366e+18 9.27135540e+00 5.00937097e-03 5.61703127e-09\n",
      "  8.00040532e-04]\n",
      " ...\n",
      " [1.17260405e+18 4.36571456e+00 1.53623764e-03 3.22678300e-10\n",
      "  2.46050173e-04]\n",
      " [5.72880419e+18 7.53065295e+00 1.24412796e-02 2.96096536e-08\n",
      "  1.95513543e-03]\n",
      " [8.55078529e+18 9.70339825e+00 1.75023633e-02 7.34022430e-08\n",
      "  2.74680183e-03]]\n",
      "[[4.33206941e+18 4.13348006e+00 1.57549986e-02 3.08487628e-08\n",
      "  2.43827849e-03]\n",
      " [7.94411587e+18 2.76658582e+00 6.58601168e-02 4.18668238e-07\n",
      "  9.71365427e-03]\n",
      " [5.94227492e+18 6.59480259e+00 1.57776422e-02 4.30118443e-08\n",
      "  2.46301003e-03]\n",
      " ...\n",
      " [5.73362811e+18 8.60258979e+00 1.03812396e-02 2.29250478e-08\n",
      "  1.63990717e-03]\n",
      " [1.05630036e+18 7.47518077e+00 6.65338584e-04 8.74046945e-11\n",
      "  1.07899169e-04]\n",
      " [6.50991143e+18 6.57047545e+00 1.85702376e-02 5.95510623e-08\n",
      "  2.88959944e-03]]\n",
      "First element: [3.53845359e+18 4.22564313e+00 1.08354926e-02 1.48561611e-08\n",
      " 1.68981144e-03]\n"
     ]
    }
   ],
   "source": [
    "#Read columns\n",
    "\n",
    "intens = h5File['Intensity_(W_cm2)']\n",
    "\n",
    "duration = h5File['Pulse_Duration_(fs)']\n",
    "\n",
    "thickness = h5File['Target_Thickness (um)']\n",
    "\n",
    "spotSize = h5File['Spot_Size_(FWHM um)']\n",
    "\n",
    "maxEnergy = h5File['Max_Proton_Energy_(MeV)']\n",
    "\n",
    "totalEnergy = h5File['Total_Proton_Energy_(MeV)']\n",
    "\n",
    "avgEnergy = h5File['Avg_Proton_Energy_(MeV)']\n",
    "\n",
    "test = zip(intens, duration, thickness, spotSize, maxEnergy, totalEnergy, avgEnergy)\n",
    "\n",
    "print(next(test)) #Prints the first row from the h5 file\n",
    "\n",
    "nextRow = next(test)\n",
    "\n",
    "print(type(nextRow))\n",
    "\n",
    "#Convert columns into numpy arrays\n",
    "npIntens = np.fromiter(intens, float)\n",
    "npDuration = np.fromiter(duration, float)\n",
    "npThickness = np.fromiter(thickness, float)\n",
    "npSpot = np.fromiter(spotSize, float)\n",
    "npMaxEnergy = np.fromiter(maxEnergy, float)\n",
    "npTotalEnergy = np.fromiter(totalEnergy, float)\n",
    "npAvgEnergy = np.fromiter(avgEnergy, float)\n",
    "\n",
    "#print(npIntens)\n",
    "\n",
    "print(npIntens.shape)\n",
    "\n",
    "#Join all of those arrays into one big numpy array\n",
    "npFile = np.dstack((npIntens, npDuration, npThickness, npSpot, npMaxEnergy, npTotalEnergy, npAvgEnergy))\n",
    "\n",
    "#Two input version\n",
    "npFile = np.dstack((npIntens, npThickness, npMaxEnergy, npTotalEnergy, npAvgEnergy))\n",
    "\n",
    "print(npFile.shape)\n",
    "\n",
    "#npFile = npFile.reshape(10000, 7)\n",
    "\n",
    "#Two input version\n",
    "npFile = npFile.reshape(10000, 5)\n",
    "\n",
    "print(npFile.shape)\n",
    "\n",
    "print(\"Average Energy:\", npAvgEnergy)\n",
    "print(\"Total Energy:\", npTotalEnergy)\n",
    "\n",
    "print(npFile)\n",
    "\n",
    "#npTrain = npFile[:9000, 0:7]\n",
    "#npTest = npFile[9000:, 0:7]\n",
    "\n",
    "npTrain = npFile[:9000, 0:5]\n",
    "npTest = npFile[9000:, 0:5]\n",
    "\n",
    "print(npTrain)\n",
    "print(npTest)\n",
    "print(\"First element:\", npTrain[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out one of the columns\n",
    "\n",
    "intens[:]\n",
    "\n",
    "#Can we convert intens by a log\n",
    "logIntens = np.log(intens)\n",
    "\n",
    "print(intens[:])\n",
    "print(logIntens[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out every value in a column\n",
    "\n",
    "# print(type(totalEnergy))\n",
    "\n",
    "# for i in range(len(intens)):\n",
    "#     print(intens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.norm0 = nn.BatchNorm1d(2)\n",
    "    self.linear1 = nn.Linear(in_features=2, out_features=64)\n",
    "    self.norm1 = nn.BatchNorm1d(64)\n",
    "    self.act1 = nn.LeakyReLU()\n",
    "    self.linear2 = nn.Linear(in_features=64, out_features=16)\n",
    "    self.norm2 = nn.BatchNorm1d(16)\n",
    "    self.act2 = nn.LeakyReLU()\n",
    "    self.linear3 = nn.Linear(in_features=16, out_features=16)\n",
    "    self.norm3 = nn.BatchNorm1d(16)\n",
    "    self.act3 = nn.LeakyReLU()\n",
    "    self.output = nn.Linear(in_features=16, out_features = 3)\n",
    "    \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    x = self.norm0(x)\n",
    "    x = self.linear1(x)\n",
    "    x = self.norm1(x)\n",
    "    x = self.act1(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.norm2(x)\n",
    "    x = self.act2(x)\n",
    "    x = self.output(x)\n",
    "    \n",
    "    \n",
    "    return x\n",
    "\n",
    "class MultiRegressor1Layer(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.norm0 = nn.BatchNorm1d(2)\n",
    "    self.linear1 = nn.Linear(in_features=2, out_features=64)\n",
    "    self.norm1 = nn.BatchNorm1d(64)\n",
    "    self.act1 = nn.LeakyReLU()\n",
    "    self.dropout = nn.Dropout()\n",
    "    self.linear2 = nn.Linear(in_features=64, out_features=16)\n",
    "    self.norm2 = nn.BatchNorm1d(16)\n",
    "    #self.dropout = nn.Dropout()\n",
    "    self.act2 = nn.LeakyReLU()\n",
    "    self.output = nn.Linear(in_features=16, out_features = 3)\n",
    "    \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    x = self.norm0(x)\n",
    "    x = self.linear1(x)\n",
    "    x = self.norm1(x)\n",
    "    x = self.act1(x)\n",
    "    #x = self.dropout(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.norm2(x)\n",
    "    #x = self.dropout(x)\n",
    "    x = self.act2(x)\n",
    "    x = self.output(x)\n",
    "    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the data\n",
    "#Unfamiliar with h5py, Hopefully this doesn't break anything\n",
    "\n",
    "#h5File.close()\n",
    "\n",
    "\n",
    "\n",
    "#f = h5py.File('test.hdf5', 'w')\n",
    "#f = h5py.File('test', 'r+')\n",
    "\n",
    "training_dataset = h5File.create_dataset(name=None, data=npTrain)\n",
    "\n",
    "\n",
    "#print(list(h5File.keys()))\n",
    "#test = h5File[0:7]\n",
    "#test = h5File['Intensity_(W_cm2)','Pulse_Duration_(fs)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will the PyTorch DataLoader class work with H5Py datasets?\n",
    "\n",
    "dataloader = DataLoader(training_dataset, batch_size=batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.3120e+18, 2.3433e+00, 3.7639e-02, 1.2655e-07, 5.6370e-03],\n",
      "        [2.9343e+18, 7.2526e+00, 4.0928e-03, 3.0952e-09, 6.5295e-04],\n",
      "        [9.2354e+18, 2.1362e+00, 1.0447e-01, 9.1258e-07, 1.5034e-02],\n",
      "        [1.3033e+18, 1.3505e+00, 4.7316e-03, 1.7626e-09, 7.3732e-04],\n",
      "        [3.3213e+18, 1.7058e+00, 2.0882e-02, 3.4547e-08, 3.1632e-03],\n",
      "        [5.3196e+18, 7.3999e+00, 1.1199e-02, 2.3620e-08, 1.7624e-03],\n",
      "        [7.5263e+18, 7.1133e+00, 2.1546e-02, 8.5667e-08, 3.3478e-03],\n",
      "        [3.0556e+18, 6.8610e+00, 4.7240e-03, 3.9547e-09, 7.5174e-04],\n",
      "        [8.9581e+18, 6.1433e+00, 3.5145e-02, 2.0450e-07, 5.3831e-03],\n",
      "        [8.7435e+18, 4.5280e+00, 4.8162e-02, 3.0637e-07, 7.2613e-03],\n",
      "        [6.3940e+18, 7.8529e+00, 1.4226e-02, 4.0162e-08, 2.2322e-03],\n",
      "        [4.5690e+18, 9.8781e+00, 5.7432e-03, 7.7972e-09, 9.1658e-04],\n",
      "        [1.9463e+18, 9.7007e+00, 1.3376e-03, 4.1700e-10, 2.1643e-04],\n",
      "        [3.6454e+18, 8.8777e+00, 4.5220e-03, 4.4156e-09, 7.2263e-04],\n",
      "        [4.5765e+18, 5.4716e+00, 1.2638e-02, 2.3981e-08, 1.9743e-03],\n",
      "        [3.0864e+18, 6.1319e+00, 5.5486e-03, 5.0266e-09, 8.7967e-04],\n",
      "        [1.2808e+18, 9.2109e+00, 6.9620e-04, 1.1050e-10, 1.1310e-04],\n",
      "        [7.0744e+18, 2.0665e+00, 6.7690e-02, 3.8064e-07, 9.9101e-03],\n",
      "        [3.6088e+18, 1.5043e+00, 2.6006e-02, 5.0767e-08, 3.9087e-03],\n",
      "        [6.1905e+18, 8.4687e+00, 1.2122e-02, 3.0949e-08, 1.9097e-03],\n",
      "        [9.1132e+18, 8.3049e-01, 1.6766e-01, 1.6383e-06, 2.3124e-02],\n",
      "        [4.8331e+18, 8.3888e+00, 7.9916e-03, 1.3246e-08, 1.2668e-03],\n",
      "        [5.0491e+18, 5.1623e+00, 1.6052e-02, 3.7138e-08, 2.4936e-03],\n",
      "        [2.6075e+18, 4.6766e+00, 5.7086e-03, 4.4407e-09, 9.0101e-04],\n",
      "        [6.0113e+18, 5.4502e+00, 2.0375e-02, 6.2272e-08, 3.1527e-03],\n",
      "        [2.3105e+18, 2.1510e+00, 9.5061e-03, 8.0885e-09, 1.4707e-03],\n",
      "        [1.6634e+18, 3.0529e+00, 4.0305e-03, 1.7652e-09, 6.3550e-04],\n",
      "        [1.2215e+18, 9.9943e+00, 5.6956e-04, 7.9014e-11, 9.2707e-05],\n",
      "        [1.8438e+18, 8.4349e-01, 1.0680e-02, 7.6139e-09, 1.6313e-03],\n",
      "        [1.3978e+18, 2.6308e+00, 3.3923e-03, 1.1758e-09, 5.3515e-04],\n",
      "        [3.5157e+18, 5.3790e+00, 8.1614e-03, 9.8973e-09, 1.2840e-03],\n",
      "        [2.0152e+18, 5.2246e+00, 3.2129e-03, 1.5288e-09, 5.1169e-04]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[3.3820e+18, 3.0133e+00, 1.3970e-02, 2.0235e-08, 2.1555e-03],\n",
      "        [1.2843e+18, 1.1843e+00, 4.9341e-03, 1.8438e-09, 7.6741e-04],\n",
      "        [4.4975e+18, 9.3512e+00, 6.0490e-03, 8.2674e-09, 9.6399e-04],\n",
      "        [3.6228e+18, 1.3548e+00, 2.7747e-02, 5.5639e-08, 4.1565e-03],\n",
      "        [3.1038e+18, 7.1856e-01, 2.7786e-02, 4.7358e-08, 4.1322e-03],\n",
      "        [5.2974e+18, 6.1420e+00, 1.4144e-02, 3.2698e-08, 2.2096e-03],\n",
      "        [1.6913e+18, 3.5910e+00, 3.5564e-03, 1.5006e-09, 5.6289e-04],\n",
      "        [1.6904e+18, 7.2821e+00, 1.5608e-03, 4.5883e-10, 2.5142e-04],\n",
      "        [1.9502e+18, 7.1063e+00, 2.0671e-03, 7.8576e-10, 3.3205e-04],\n",
      "        [1.7957e+18, 4.0726e+00, 3.4704e-03, 1.5324e-09, 5.5036e-04],\n",
      "        [1.4330e+18, 7.7412e-01, 7.1173e-03, 3.4065e-09, 1.0957e-03],\n",
      "        [1.7526e+18, 8.4456e+00, 1.3578e-03, 3.8733e-10, 2.1935e-04],\n",
      "        [1.0955e+18, 8.6178e+00, 5.8301e-04, 7.4376e-11, 9.4755e-05],\n",
      "        [1.2779e+18, 4.0547e+00, 1.9306e-03, 4.8424e-10, 3.0824e-04],\n",
      "        [3.4067e+18, 8.6196e+00, 4.1905e-03, 3.7028e-09, 6.6996e-04],\n",
      "        [1.5067e+18, 4.9080e+00, 2.0840e-03, 6.2744e-10, 3.3322e-04],\n",
      "        [1.3174e+18, 4.6184e+00, 1.7678e-03, 4.3842e-10, 2.8292e-04],\n",
      "        [1.1705e+18, 4.6248e+00, 1.4372e-03, 2.9263e-10, 2.3048e-04],\n",
      "        [9.5395e+18, 2.8622e+00, 8.7647e-02, 7.5344e-07, 1.2796e-02],\n",
      "        [4.9699e+18, 7.6396e+00, 9.5350e-03, 1.7527e-08, 1.5052e-03],\n",
      "        [8.3280e+18, 8.0266e+00, 2.1838e-02, 9.7406e-08, 3.4005e-03],\n",
      "        [1.9874e+18, 5.0640e+00, 3.2527e-03, 1.5359e-09, 5.1775e-04],\n",
      "        [5.1340e+18, 7.1839e+00, 1.0952e-02, 2.2060e-08, 1.7233e-03],\n",
      "        [2.1839e+18, 5.1293e+00, 3.7752e-03, 2.0777e-09, 5.9994e-04],\n",
      "        [9.5339e+18, 4.0614e+00, 6.2799e-02, 4.8340e-07, 9.3633e-03],\n",
      "        [7.0214e+18, 5.4001e+00, 2.6960e-02, 1.0856e-07, 4.1447e-03],\n",
      "        [1.6751e+18, 9.6148e+00, 1.0437e-03, 2.5345e-10, 1.6917e-04],\n",
      "        [2.3196e+18, 6.6493e+00, 3.0476e-03, 1.6178e-09, 4.8723e-04],\n",
      "        [4.1236e+18, 4.6258e+00, 1.2803e-02, 2.1938e-08, 1.9935e-03],\n",
      "        [1.2527e+18, 3.3656e+00, 2.2486e-03, 5.9188e-10, 3.5762e-04],\n",
      "        [4.9857e+18, 1.6722e+00, 4.2644e-02, 1.3963e-07, 6.3225e-03],\n",
      "        [2.3234e+18, 7.1968e+00, 2.7559e-03, 1.4017e-09, 4.4153e-04]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "iterDataLoader = iter(dataloader)\n",
    "\n",
    "print(next(iterDataLoader))\n",
    "print(next(iterDataLoader))\n",
    "\n",
    "row = next(iterDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valueIntens, valueDuration, valueThickness, valueSpot, valueMax = row[0,0], row[0,1], row[0,2], row[0,3], row[0,4]\n",
    "\n",
    "# print(valueIntens, valueDuration, valueThickness, valueSpot, valueMax)\n",
    "\n",
    "# inputs = row[0, 0:4]\n",
    "\n",
    "\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test iterating through the data loader\n",
    "\n",
    "# for i, data in enumerate(dataloader, 0):\n",
    "#     #Print inputs\n",
    "#     #print(data[0])\n",
    "#     inputs = data[:, 0:4]\n",
    "#     target = data[:, 4:7]\n",
    "    \n",
    "#     print(\"Iteration:\", i)\n",
    "#     print(\"Inputs:\", inputs)\n",
    "#     print(\"Target:\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Can we apply a log to just intens section of the inputs?\n",
    "\n",
    "# copy = inputs\n",
    "# intensCopy = row[:, 0:1]\n",
    "\n",
    "# intensCopy = np.log(intensCopy)\n",
    "\n",
    "# #print(intensCopy)\n",
    "\n",
    "# #processedInput = \n",
    "\n",
    "# #Can we concatanate this with the other tensors?\n",
    "\n",
    "# print(\"Size:\", intensCopy.size())\n",
    "# print(\"Size:\", row[:,1:4].size())\n",
    "\n",
    "# concatVer = torch.cat((intensCopy, row[:, 1:4]), axis = 1)\n",
    "\n",
    "# print(concatVer)\n",
    "# print(row[:,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (norm0): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear1): Linear(in_features=2, out_features=64, bias=True)\n",
      "  (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): LeakyReLU(negative_slope=0.01)\n",
      "  (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (norm2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act2): LeakyReLU(negative_slope=0.01)\n",
      "  (linear3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (norm3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act3): LeakyReLU(negative_slope=0.01)\n",
      "  (output): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.277\n",
      "Loss after mini-batch    11: 2.648\n",
      "Loss after mini-batch    21: 2.735\n",
      "Loss after mini-batch    31: 2.685\n",
      "Loss after mini-batch    41: 2.614\n",
      "Loss after mini-batch    51: 2.619\n",
      "Loss after mini-batch    61: 2.533\n",
      "Loss after mini-batch    71: 2.476\n",
      "Loss after mini-batch    81: 2.451\n",
      "Loss after mini-batch    91: 2.421\n",
      "Loss after mini-batch   101: 2.383\n",
      "Loss after mini-batch   111: 2.296\n",
      "Loss after mini-batch   121: 2.233\n",
      "Loss after mini-batch   131: 2.217\n",
      "Loss after mini-batch   141: 2.262\n",
      "Loss after mini-batch   151: 2.106\n",
      "Loss after mini-batch   161: 2.146\n",
      "Loss after mini-batch   171: 1.993\n",
      "Loss after mini-batch   181: 1.992\n",
      "Loss after mini-batch   191: 1.981\n",
      "Loss after mini-batch   201: 1.896\n",
      "Loss after mini-batch   211: 1.879\n",
      "Loss after mini-batch   221: 1.841\n",
      "Loss after mini-batch   231: 1.685\n",
      "Loss after mini-batch   241: 1.681\n",
      "Loss after mini-batch   251: 1.678\n",
      "Loss after mini-batch   261: 1.599\n",
      "Loss after mini-batch   271: 1.506\n",
      "Loss after mini-batch   281: 1.479\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.147\n",
      "Loss after mini-batch    11: 1.420\n",
      "Loss after mini-batch    21: 1.401\n",
      "Loss after mini-batch    31: 1.304\n",
      "Loss after mini-batch    41: 1.306\n",
      "Loss after mini-batch    51: 1.263\n",
      "Loss after mini-batch    61: 1.218\n",
      "Loss after mini-batch    71: 1.164\n",
      "Loss after mini-batch    81: 1.117\n",
      "Loss after mini-batch    91: 1.049\n",
      "Loss after mini-batch   101: 0.995\n",
      "Loss after mini-batch   111: 0.968\n",
      "Loss after mini-batch   121: 0.940\n",
      "Loss after mini-batch   131: 0.878\n",
      "Loss after mini-batch   141: 0.829\n",
      "Loss after mini-batch   151: 0.796\n",
      "Loss after mini-batch   161: 0.775\n",
      "Loss after mini-batch   171: 0.782\n",
      "Loss after mini-batch   181: 0.742\n",
      "Loss after mini-batch   191: 0.655\n",
      "Loss after mini-batch   201: 0.617\n",
      "Loss after mini-batch   211: 0.621\n",
      "Loss after mini-batch   221: 0.568\n",
      "Loss after mini-batch   231: 0.503\n",
      "Loss after mini-batch   241: 0.500\n",
      "Loss after mini-batch   251: 0.467\n",
      "Loss after mini-batch   261: 0.448\n",
      "Loss after mini-batch   271: 0.419\n",
      "Loss after mini-batch   281: 0.401\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.044\n",
      "Loss after mini-batch    11: 0.381\n",
      "Loss after mini-batch    21: 0.343\n",
      "Loss after mini-batch    31: 0.329\n",
      "Loss after mini-batch    41: 0.317\n",
      "Loss after mini-batch    51: 0.288\n",
      "Loss after mini-batch    61: 0.247\n",
      "Loss after mini-batch    71: 0.224\n",
      "Loss after mini-batch    81: 0.216\n",
      "Loss after mini-batch    91: 0.192\n",
      "Loss after mini-batch   101: 0.188\n",
      "Loss after mini-batch   111: 0.170\n",
      "Loss after mini-batch   121: 0.161\n",
      "Loss after mini-batch   131: 0.150\n",
      "Loss after mini-batch   141: 0.132\n",
      "Loss after mini-batch   151: 0.116\n",
      "Loss after mini-batch   161: 0.108\n",
      "Loss after mini-batch   171: 0.104\n",
      "Loss after mini-batch   181: 0.081\n",
      "Loss after mini-batch   191: 0.082\n",
      "Loss after mini-batch   201: 0.063\n",
      "Loss after mini-batch   211: 0.070\n",
      "Loss after mini-batch   221: 0.070\n",
      "Loss after mini-batch   231: 0.048\n",
      "Loss after mini-batch   241: 0.049\n",
      "Loss after mini-batch   251: 0.048\n",
      "Loss after mini-batch   261: 0.036\n",
      "Loss after mini-batch   271: 0.040\n",
      "Loss after mini-batch   281: 0.028\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch    11: 0.028\n",
      "Loss after mini-batch    21: 0.024\n",
      "Loss after mini-batch    31: 0.026\n",
      "Loss after mini-batch    41: 0.022\n",
      "Loss after mini-batch    51: 0.027\n",
      "Loss after mini-batch    61: 0.019\n",
      "Loss after mini-batch    71: 0.018\n",
      "Loss after mini-batch    81: 0.018\n",
      "Loss after mini-batch    91: 0.018\n",
      "Loss after mini-batch   101: 0.020\n",
      "Loss after mini-batch   111: 0.012\n",
      "Loss after mini-batch   121: 0.011\n",
      "Loss after mini-batch   131: 0.012\n",
      "Loss after mini-batch   141: 0.011\n",
      "Loss after mini-batch   151: 0.010\n",
      "Loss after mini-batch   161: 0.008\n",
      "Loss after mini-batch   171: 0.008\n",
      "Loss after mini-batch   181: 0.008\n",
      "Loss after mini-batch   191: 0.008\n",
      "Loss after mini-batch   201: 0.008\n",
      "Loss after mini-batch   211: 0.008\n",
      "Loss after mini-batch   221: 0.007\n",
      "Loss after mini-batch   231: 0.007\n",
      "Loss after mini-batch   241: 0.007\n",
      "Loss after mini-batch   251: 0.006\n",
      "Loss after mini-batch   261: 0.005\n",
      "Loss after mini-batch   271: 0.005\n",
      "Loss after mini-batch   281: 0.007\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.006\n",
      "Loss after mini-batch    21: 0.006\n",
      "Loss after mini-batch    31: 0.006\n",
      "Loss after mini-batch    41: 0.005\n",
      "Loss after mini-batch    51: 0.007\n",
      "Loss after mini-batch    61: 0.009\n",
      "Loss after mini-batch    71: 0.005\n",
      "Loss after mini-batch    81: 0.007\n",
      "Loss after mini-batch    91: 0.006\n",
      "Loss after mini-batch   101: 0.005\n",
      "Loss after mini-batch   111: 0.006\n",
      "Loss after mini-batch   121: 0.005\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.005\n",
      "Loss after mini-batch   151: 0.005\n",
      "Loss after mini-batch   161: 0.006\n",
      "Loss after mini-batch   171: 0.004\n",
      "Loss after mini-batch   181: 0.004\n",
      "Loss after mini-batch   191: 0.004\n",
      "Loss after mini-batch   201: 0.004\n",
      "Loss after mini-batch   211: 0.005\n",
      "Loss after mini-batch   221: 0.006\n",
      "Loss after mini-batch   231: 0.004\n",
      "Loss after mini-batch   241: 0.004\n",
      "Loss after mini-batch   251: 0.004\n",
      "Loss after mini-batch   261: 0.005\n",
      "Loss after mini-batch   271: 0.003\n",
      "Loss after mini-batch   281: 0.004\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch    11: 0.006\n",
      "Loss after mini-batch    21: 0.008\n",
      "Loss after mini-batch    31: 0.007\n",
      "Loss after mini-batch    41: 0.005\n",
      "Loss after mini-batch    51: 0.005\n",
      "Loss after mini-batch    61: 0.005\n",
      "Loss after mini-batch    71: 0.004\n",
      "Loss after mini-batch    81: 0.007\n",
      "Loss after mini-batch    91: 0.006\n",
      "Loss after mini-batch   101: 0.004\n",
      "Loss after mini-batch   111: 0.005\n",
      "Loss after mini-batch   121: 0.006\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.005\n",
      "Loss after mini-batch   151: 0.006\n",
      "Loss after mini-batch   161: 0.004\n",
      "Loss after mini-batch   171: 0.007\n",
      "Loss after mini-batch   181: 0.004\n",
      "Loss after mini-batch   191: 0.007\n",
      "Loss after mini-batch   201: 0.009\n",
      "Loss after mini-batch   211: 0.005\n",
      "Loss after mini-batch   221: 0.006\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.007\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.004\n",
      "Loss after mini-batch   271: 0.004\n",
      "Loss after mini-batch   281: 0.005\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch    11: 0.005\n",
      "Loss after mini-batch    21: 0.004\n",
      "Loss after mini-batch    31: 0.005\n",
      "Loss after mini-batch    41: 0.005\n",
      "Loss after mini-batch    51: 0.003\n",
      "Loss after mini-batch    61: 0.005\n",
      "Loss after mini-batch    71: 0.005\n",
      "Loss after mini-batch    81: 0.004\n",
      "Loss after mini-batch    91: 0.004\n",
      "Loss after mini-batch   101: 0.005\n",
      "Loss after mini-batch   111: 0.005\n",
      "Loss after mini-batch   121: 0.003\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.004\n",
      "Loss after mini-batch   151: 0.004\n",
      "Loss after mini-batch   161: 0.003\n",
      "Loss after mini-batch   171: 0.004\n",
      "Loss after mini-batch   181: 0.003\n",
      "Loss after mini-batch   191: 0.003\n",
      "Loss after mini-batch   201: 0.004\n",
      "Loss after mini-batch   211: 0.004\n",
      "Loss after mini-batch   221: 0.004\n",
      "Loss after mini-batch   231: 0.004\n",
      "Loss after mini-batch   241: 0.003\n",
      "Loss after mini-batch   251: 0.004\n",
      "Loss after mini-batch   261: 0.004\n",
      "Loss after mini-batch   271: 0.003\n",
      "Loss after mini-batch   281: 0.004\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.004\n",
      "Loss after mini-batch    21: 0.004\n",
      "Loss after mini-batch    31: 0.002\n",
      "Loss after mini-batch    41: 0.002\n",
      "Loss after mini-batch    51: 0.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch    61: 0.003\n",
      "Loss after mini-batch    71: 0.005\n",
      "Loss after mini-batch    81: 0.004\n",
      "Loss after mini-batch    91: 0.005\n",
      "Loss after mini-batch   101: 0.006\n",
      "Loss after mini-batch   111: 0.005\n",
      "Loss after mini-batch   121: 0.003\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.006\n",
      "Loss after mini-batch   151: 0.004\n",
      "Loss after mini-batch   161: 0.005\n",
      "Loss after mini-batch   171: 0.004\n",
      "Loss after mini-batch   181: 0.004\n",
      "Loss after mini-batch   191: 0.005\n",
      "Loss after mini-batch   201: 0.004\n",
      "Loss after mini-batch   211: 0.003\n",
      "Loss after mini-batch   221: 0.002\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.003\n",
      "Loss after mini-batch   251: 0.004\n",
      "Loss after mini-batch   261: 0.004\n",
      "Loss after mini-batch   271: 0.004\n",
      "Loss after mini-batch   281: 0.003\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.006\n",
      "Loss after mini-batch    21: 0.005\n",
      "Loss after mini-batch    31: 0.006\n",
      "Loss after mini-batch    41: 0.005\n",
      "Loss after mini-batch    51: 0.002\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.002\n",
      "Loss after mini-batch    81: 0.005\n",
      "Loss after mini-batch    91: 0.003\n",
      "Loss after mini-batch   101: 0.004\n",
      "Loss after mini-batch   111: 0.004\n",
      "Loss after mini-batch   121: 0.004\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.005\n",
      "Loss after mini-batch   151: 0.003\n",
      "Loss after mini-batch   161: 0.003\n",
      "Loss after mini-batch   171: 0.006\n",
      "Loss after mini-batch   181: 0.004\n",
      "Loss after mini-batch   191: 0.003\n",
      "Loss after mini-batch   201: 0.004\n",
      "Loss after mini-batch   211: 0.004\n",
      "Loss after mini-batch   221: 0.003\n",
      "Loss after mini-batch   231: 0.007\n",
      "Loss after mini-batch   241: 0.003\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.003\n",
      "Loss after mini-batch   281: 0.003\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.005\n",
      "Loss after mini-batch    41: 0.002\n",
      "Loss after mini-batch    51: 0.004\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.002\n",
      "Loss after mini-batch    81: 0.004\n",
      "Loss after mini-batch    91: 0.006\n",
      "Loss after mini-batch   101: 0.003\n",
      "Loss after mini-batch   111: 0.004\n",
      "Loss after mini-batch   121: 0.005\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.007\n",
      "Loss after mini-batch   151: 0.004\n",
      "Loss after mini-batch   161: 0.006\n",
      "Loss after mini-batch   171: 0.004\n",
      "Loss after mini-batch   181: 0.005\n",
      "Loss after mini-batch   191: 0.004\n",
      "Loss after mini-batch   201: 0.003\n",
      "Loss after mini-batch   211: 0.004\n",
      "Loss after mini-batch   221: 0.003\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.004\n",
      "Loss after mini-batch   251: 0.004\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.004\n",
      "Loss after mini-batch   281: 0.005\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.005\n",
      "Loss after mini-batch    21: 0.005\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.005\n",
      "Loss after mini-batch    51: 0.004\n",
      "Loss after mini-batch    61: 0.003\n",
      "Loss after mini-batch    71: 0.004\n",
      "Loss after mini-batch    81: 0.004\n",
      "Loss after mini-batch    91: 0.003\n",
      "Loss after mini-batch   101: 0.004\n",
      "Loss after mini-batch   111: 0.004\n",
      "Loss after mini-batch   121: 0.005\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.005\n",
      "Loss after mini-batch   151: 0.004\n",
      "Loss after mini-batch   161: 0.004\n",
      "Loss after mini-batch   171: 0.003\n",
      "Loss after mini-batch   181: 0.004\n",
      "Loss after mini-batch   191: 0.004\n",
      "Loss after mini-batch   201: 0.005\n",
      "Loss after mini-batch   211: 0.003\n",
      "Loss after mini-batch   221: 0.002\n",
      "Loss after mini-batch   231: 0.004\n",
      "Loss after mini-batch   241: 0.002\n",
      "Loss after mini-batch   251: 0.005\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.004\n",
      "Loss after mini-batch   281: 0.002\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.002\n",
      "Loss after mini-batch    21: 0.007\n",
      "Loss after mini-batch    31: 0.004\n",
      "Loss after mini-batch    41: 0.003\n",
      "Loss after mini-batch    51: 0.005\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.005\n",
      "Loss after mini-batch    81: 0.004\n",
      "Loss after mini-batch    91: 0.004\n",
      "Loss after mini-batch   101: 0.002\n",
      "Loss after mini-batch   111: 0.004\n",
      "Loss after mini-batch   121: 0.003\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.004\n",
      "Loss after mini-batch   151: 0.003\n",
      "Loss after mini-batch   161: 0.004\n",
      "Loss after mini-batch   171: 0.002\n",
      "Loss after mini-batch   181: 0.003\n",
      "Loss after mini-batch   191: 0.006\n",
      "Loss after mini-batch   201: 0.004\n",
      "Loss after mini-batch   211: 0.003\n",
      "Loss after mini-batch   221: 0.002\n",
      "Loss after mini-batch   231: 0.004\n",
      "Loss after mini-batch   241: 0.002\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.004\n",
      "Loss after mini-batch   271: 0.003\n",
      "Loss after mini-batch   281: 0.003\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.003\n",
      "Loss after mini-batch    51: 0.003\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.005\n",
      "Loss after mini-batch    81: 0.004\n",
      "Loss after mini-batch    91: 0.003\n",
      "Loss after mini-batch   101: 0.003\n",
      "Loss after mini-batch   111: 0.004\n",
      "Loss after mini-batch   121: 0.005\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.003\n",
      "Loss after mini-batch   151: 0.003\n",
      "Loss after mini-batch   161: 0.003\n",
      "Loss after mini-batch   171: 0.002\n",
      "Loss after mini-batch   181: 0.002\n",
      "Loss after mini-batch   191: 0.005\n",
      "Loss after mini-batch   201: 0.003\n",
      "Loss after mini-batch   211: 0.003\n",
      "Loss after mini-batch   221: 0.005\n",
      "Loss after mini-batch   231: 0.004\n",
      "Loss after mini-batch   241: 0.003\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.004\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.002\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.002\n",
      "Loss after mini-batch    21: 0.004\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.003\n",
      "Loss after mini-batch    51: 0.005\n",
      "Loss after mini-batch    61: 0.003\n",
      "Loss after mini-batch    71: 0.003\n",
      "Loss after mini-batch    81: 0.003\n",
      "Loss after mini-batch    91: 0.004\n",
      "Loss after mini-batch   101: 0.004\n",
      "Loss after mini-batch   111: 0.003\n",
      "Loss after mini-batch   121: 0.006\n",
      "Loss after mini-batch   131: 0.005\n",
      "Loss after mini-batch   141: 0.003\n",
      "Loss after mini-batch   151: 0.003\n",
      "Loss after mini-batch   161: 0.003\n",
      "Loss after mini-batch   171: 0.002\n",
      "Loss after mini-batch   181: 0.003\n",
      "Loss after mini-batch   191: 0.004\n",
      "Loss after mini-batch   201: 0.004\n",
      "Loss after mini-batch   211: 0.002\n",
      "Loss after mini-batch   221: 0.004\n",
      "Loss after mini-batch   231: 0.004\n",
      "Loss after mini-batch   241: 0.003\n",
      "Loss after mini-batch   251: 0.002\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.004\n",
      "Loss after mini-batch   281: 0.003\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.004\n",
      "Loss after mini-batch    51: 0.004\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.005\n",
      "Loss after mini-batch    81: 0.003\n",
      "Loss after mini-batch    91: 0.003\n",
      "Loss after mini-batch   101: 0.004\n",
      "Loss after mini-batch   111: 0.003\n",
      "Loss after mini-batch   121: 0.003\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.003\n",
      "Loss after mini-batch   151: 0.004\n",
      "Loss after mini-batch   161: 0.004\n",
      "Loss after mini-batch   171: 0.006\n",
      "Loss after mini-batch   181: 0.004\n",
      "Loss after mini-batch   191: 0.003\n",
      "Loss after mini-batch   201: 0.003\n",
      "Loss after mini-batch   211: 0.003\n",
      "Loss after mini-batch   221: 0.003\n",
      "Loss after mini-batch   231: 0.002\n",
      "Loss after mini-batch   241: 0.002\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.003\n",
      "Loss after mini-batch   281: 0.003\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch    41: 0.004\n",
      "Loss after mini-batch    51: 0.004\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.003\n",
      "Loss after mini-batch    81: 0.003\n",
      "Loss after mini-batch    91: 0.003\n",
      "Loss after mini-batch   101: 0.003\n",
      "Loss after mini-batch   111: 0.002\n",
      "Loss after mini-batch   121: 0.003\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.004\n",
      "Loss after mini-batch   151: 0.007\n",
      "Loss after mini-batch   161: 0.005\n",
      "Loss after mini-batch   171: 0.003\n",
      "Loss after mini-batch   181: 0.003\n",
      "Loss after mini-batch   191: 0.003\n",
      "Loss after mini-batch   201: 0.001\n",
      "Loss after mini-batch   211: 0.002\n",
      "Loss after mini-batch   221: 0.004\n",
      "Loss after mini-batch   231: 0.004\n",
      "Loss after mini-batch   241: 0.004\n",
      "Loss after mini-batch   251: 0.002\n",
      "Loss after mini-batch   261: 0.004\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.002\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.002\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.003\n",
      "Loss after mini-batch    51: 0.002\n",
      "Loss after mini-batch    61: 0.002\n",
      "Loss after mini-batch    71: 0.003\n",
      "Loss after mini-batch    81: 0.005\n",
      "Loss after mini-batch    91: 0.005\n",
      "Loss after mini-batch   101: 0.003\n",
      "Loss after mini-batch   111: 0.005\n",
      "Loss after mini-batch   121: 0.004\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.005\n",
      "Loss after mini-batch   151: 0.004\n",
      "Loss after mini-batch   161: 0.003\n",
      "Loss after mini-batch   171: 0.003\n",
      "Loss after mini-batch   181: 0.002\n",
      "Loss after mini-batch   191: 0.002\n",
      "Loss after mini-batch   201: 0.004\n",
      "Loss after mini-batch   211: 0.005\n",
      "Loss after mini-batch   221: 0.004\n",
      "Loss after mini-batch   231: 0.002\n",
      "Loss after mini-batch   241: 0.004\n",
      "Loss after mini-batch   251: 0.004\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.004\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.002\n",
      "Loss after mini-batch    51: 0.002\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.002\n",
      "Loss after mini-batch    81: 0.003\n",
      "Loss after mini-batch    91: 0.002\n",
      "Loss after mini-batch   101: 0.002\n",
      "Loss after mini-batch   111: 0.004\n",
      "Loss after mini-batch   121: 0.003\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.002\n",
      "Loss after mini-batch   151: 0.002\n",
      "Loss after mini-batch   161: 0.003\n",
      "Loss after mini-batch   171: 0.004\n",
      "Loss after mini-batch   181: 0.003\n",
      "Loss after mini-batch   191: 0.002\n",
      "Loss after mini-batch   201: 0.003\n",
      "Loss after mini-batch   211: 0.004\n",
      "Loss after mini-batch   221: 0.004\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.005\n",
      "Loss after mini-batch   251: 0.002\n",
      "Loss after mini-batch   261: 0.004\n",
      "Loss after mini-batch   271: 0.001\n",
      "Loss after mini-batch   281: 0.004\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.002\n",
      "Loss after mini-batch    41: 0.004\n",
      "Loss after mini-batch    51: 0.005\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.004\n",
      "Loss after mini-batch    81: 0.002\n",
      "Loss after mini-batch    91: 0.004\n",
      "Loss after mini-batch   101: 0.003\n",
      "Loss after mini-batch   111: 0.002\n",
      "Loss after mini-batch   121: 0.003\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.003\n",
      "Loss after mini-batch   151: 0.002\n",
      "Loss after mini-batch   161: 0.004\n",
      "Loss after mini-batch   171: 0.004\n",
      "Loss after mini-batch   181: 0.003\n",
      "Loss after mini-batch   191: 0.003\n",
      "Loss after mini-batch   201: 0.002\n",
      "Loss after mini-batch   211: 0.002\n",
      "Loss after mini-batch   221: 0.002\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.002\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.003\n",
      "Loss after mini-batch   281: 0.002\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.002\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.004\n",
      "Loss after mini-batch    51: 0.004\n",
      "Loss after mini-batch    61: 0.002\n",
      "Loss after mini-batch    71: 0.003\n",
      "Loss after mini-batch    81: 0.003\n",
      "Loss after mini-batch    91: 0.002\n",
      "Loss after mini-batch   101: 0.002\n",
      "Loss after mini-batch   111: 0.002\n",
      "Loss after mini-batch   121: 0.001\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.005\n",
      "Loss after mini-batch   151: 0.003\n",
      "Loss after mini-batch   161: 0.003\n",
      "Loss after mini-batch   171: 0.003\n",
      "Loss after mini-batch   181: 0.004\n",
      "Loss after mini-batch   191: 0.005\n",
      "Loss after mini-batch   201: 0.005\n",
      "Loss after mini-batch   211: 0.003\n",
      "Loss after mini-batch   221: 0.002\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.004\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.004\n",
      "Starting epoch 21\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.005\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.003\n",
      "Loss after mini-batch    51: 0.005\n",
      "Loss after mini-batch    61: 0.002\n",
      "Loss after mini-batch    71: 0.002\n",
      "Loss after mini-batch    81: 0.004\n",
      "Loss after mini-batch    91: 0.002\n",
      "Loss after mini-batch   101: 0.002\n",
      "Loss after mini-batch   111: 0.003\n",
      "Loss after mini-batch   121: 0.003\n",
      "Loss after mini-batch   131: 0.002\n",
      "Loss after mini-batch   141: 0.004\n",
      "Loss after mini-batch   151: 0.002\n",
      "Loss after mini-batch   161: 0.004\n",
      "Loss after mini-batch   171: 0.004\n",
      "Loss after mini-batch   181: 0.002\n",
      "Loss after mini-batch   191: 0.003\n",
      "Loss after mini-batch   201: 0.004\n",
      "Loss after mini-batch   211: 0.005\n",
      "Loss after mini-batch   221: 0.004\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.003\n",
      "Loss after mini-batch   251: 0.002\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.002\n",
      "Starting epoch 22\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.004\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.004\n",
      "Loss after mini-batch    51: 0.002\n",
      "Loss after mini-batch    61: 0.002\n",
      "Loss after mini-batch    71: 0.004\n",
      "Loss after mini-batch    81: 0.002\n",
      "Loss after mini-batch    91: 0.002\n",
      "Loss after mini-batch   101: 0.003\n",
      "Loss after mini-batch   111: 0.002\n",
      "Loss after mini-batch   121: 0.002\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.002\n",
      "Loss after mini-batch   151: 0.004\n",
      "Loss after mini-batch   161: 0.004\n",
      "Loss after mini-batch   171: 0.002\n",
      "Loss after mini-batch   181: 0.002\n",
      "Loss after mini-batch   191: 0.002\n",
      "Loss after mini-batch   201: 0.003\n",
      "Loss after mini-batch   211: 0.003\n",
      "Loss after mini-batch   221: 0.002\n",
      "Loss after mini-batch   231: 0.002\n",
      "Loss after mini-batch   241: 0.002\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.002\n",
      "Starting epoch 23\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch    11: 0.004\n",
      "Loss after mini-batch    21: 0.003\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.003\n",
      "Loss after mini-batch    51: 0.003\n",
      "Loss after mini-batch    61: 0.003\n",
      "Loss after mini-batch    71: 0.003\n",
      "Loss after mini-batch    81: 0.003\n",
      "Loss after mini-batch    91: 0.007\n",
      "Loss after mini-batch   101: 0.002\n",
      "Loss after mini-batch   111: 0.001\n",
      "Loss after mini-batch   121: 0.002\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.002\n",
      "Loss after mini-batch   151: 0.004\n",
      "Loss after mini-batch   161: 0.002\n",
      "Loss after mini-batch   171: 0.003\n",
      "Loss after mini-batch   181: 0.002\n",
      "Loss after mini-batch   191: 0.003\n",
      "Loss after mini-batch   201: 0.002\n",
      "Loss after mini-batch   211: 0.002\n",
      "Loss after mini-batch   221: 0.002\n",
      "Loss after mini-batch   231: 0.002\n",
      "Loss after mini-batch   241: 0.002\n",
      "Loss after mini-batch   251: 0.002\n",
      "Loss after mini-batch   261: 0.004\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.002\n",
      "Starting epoch 24\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.002\n",
      "Loss after mini-batch    21: 0.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch    31: 0.004\n",
      "Loss after mini-batch    41: 0.002\n",
      "Loss after mini-batch    51: 0.003\n",
      "Loss after mini-batch    61: 0.004\n",
      "Loss after mini-batch    71: 0.003\n",
      "Loss after mini-batch    81: 0.002\n",
      "Loss after mini-batch    91: 0.003\n",
      "Loss after mini-batch   101: 0.003\n",
      "Loss after mini-batch   111: 0.003\n",
      "Loss after mini-batch   121: 0.004\n",
      "Loss after mini-batch   131: 0.003\n",
      "Loss after mini-batch   141: 0.003\n",
      "Loss after mini-batch   151: 0.003\n",
      "Loss after mini-batch   161: 0.002\n",
      "Loss after mini-batch   171: 0.003\n",
      "Loss after mini-batch   181: 0.002\n",
      "Loss after mini-batch   191: 0.004\n",
      "Loss after mini-batch   201: 0.003\n",
      "Loss after mini-batch   211: 0.003\n",
      "Loss after mini-batch   221: 0.003\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.003\n",
      "Loss after mini-batch   251: 0.002\n",
      "Loss after mini-batch   261: 0.003\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.004\n",
      "Starting epoch 25\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch    11: 0.003\n",
      "Loss after mini-batch    21: 0.002\n",
      "Loss after mini-batch    31: 0.003\n",
      "Loss after mini-batch    41: 0.002\n",
      "Loss after mini-batch    51: 0.004\n",
      "Loss after mini-batch    61: 0.003\n",
      "Loss after mini-batch    71: 0.002\n",
      "Loss after mini-batch    81: 0.003\n",
      "Loss after mini-batch    91: 0.002\n",
      "Loss after mini-batch   101: 0.002\n",
      "Loss after mini-batch   111: 0.002\n",
      "Loss after mini-batch   121: 0.002\n",
      "Loss after mini-batch   131: 0.004\n",
      "Loss after mini-batch   141: 0.002\n",
      "Loss after mini-batch   151: 0.001\n",
      "Loss after mini-batch   161: 0.001\n",
      "Loss after mini-batch   171: 0.002\n",
      "Loss after mini-batch   181: 0.002\n",
      "Loss after mini-batch   191: 0.002\n",
      "Loss after mini-batch   201: 0.002\n",
      "Loss after mini-batch   211: 0.002\n",
      "Loss after mini-batch   221: 0.003\n",
      "Loss after mini-batch   231: 0.003\n",
      "Loss after mini-batch   241: 0.004\n",
      "Loss after mini-batch   251: 0.003\n",
      "Loss after mini-batch   261: 0.002\n",
      "Loss after mini-batch   271: 0.002\n",
      "Loss after mini-batch   281: 0.002\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "#Initialize neural network\n",
    "\n",
    "#model = MLP()\n",
    "model = MLP().to('cuda') #GPU Version, comment out if not using GPU\n",
    "#model = MultiRegressor1Layer().to('cuda')\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "numEpochs = 25\n",
    "\n",
    "print(model)\n",
    "\n",
    "trainNetwork(model, loss_function, optimizer, numEpochs, dataloader, numInputs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (norm0): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=2, out_features=64, bias=True)\n",
       "  (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): LeakyReLU(negative_slope=0.01)\n",
       "  (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (norm2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): LeakyReLU(negative_slope=0.01)\n",
       "  (linear3): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (norm3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): LeakyReLU(negative_slope=0.01)\n",
       "  (output): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a path\n",
    "PATH = \"test_regression.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0222e+18, 4.2750e+00, 8.1394e-03, 8.4813e-09, 1.2759e-03],\n",
      "        [2.2372e+18, 5.9951e+00, 3.2626e-03, 1.7244e-09, 5.2049e-04],\n",
      "        [5.4289e+18, 9.3179e+00, 8.4318e-03, 1.6109e-08, 1.3378e-03],\n",
      "        ...,\n",
      "        [1.2727e+18, 1.6589e+00, 4.0307e-03, 1.3762e-09, 6.3101e-04],\n",
      "        [6.6212e+18, 9.7405e+00, 1.1166e-02, 2.9553e-08, 1.7655e-03],\n",
      "        [9.0304e+18, 6.1465e+00, 3.5614e-02, 2.1014e-07, 5.4531e-03]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 4 elements not 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-68ad2ca8f4d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#print(output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7081e7ae024b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m       \u001b[0mForward\u001b[0m \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     '''\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \"\"\"\n\u001b[1;32m--> 168\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2419\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2421\u001b[1;33m     return torch.batch_norm(\n\u001b[0m\u001b[0;32m   2422\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2423\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 4 elements not 2"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Let's put in the training dataset in now\n",
    "\n",
    "test_dataset = h5File.create_dataset(name=None, data=npTest)\n",
    "testDataloader = DataLoader(test_dataset, batch_size=500, shuffle=True)\n",
    "iterDataLoader = iter(testDataloader)\n",
    "testData = next(iterDataLoader)\n",
    "\n",
    "#Originally had tested with the full dataset\n",
    "# fulldata = DataLoader(training_dataset, batch_size=5000, shuffle=True)\n",
    "# iterDataLoader = iter(fulldata)\n",
    "# allData = next(iterDataLoader)\n",
    "\n",
    "print(testData)\n",
    "\n",
    "#Get each separate input for future plotting reasons\n",
    "intens = testData[:, 0:1]\n",
    "duration = testData[:, 1]\n",
    "thickness = testData[:, 2]\n",
    "spotSize = testData[:, 3]\n",
    "\n",
    "#Transform intens by a log function\n",
    "logIntens = np.log(intens)\n",
    "\n",
    "#print(logIntens.size(), allData[:,1:4].size())\n",
    "#Process intensity by putting it on a log scale\n",
    "inputs = torch.cat((logIntens, testData[:,1:4]), axis = 1)\n",
    "\n",
    "#Turn logIntens back into a tensor that can be passed to a plot\n",
    "intens = testData[:, 0]\n",
    "logIntens = np.log(intens)\n",
    "\n",
    "target = testData[:, 4]\n",
    "\n",
    "inputs = inputs.to('cuda')\n",
    "target = target.to('cuda')\n",
    "\n",
    "inputs, target = inputs.float(), target.float()\n",
    "target = target.reshape((target.shape[0], 1))\n",
    "\n",
    "output = model(inputs)\n",
    "\n",
    "#print(output)\n",
    "print(output)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot just actual data itself\n",
    "\n",
    "print(target.size())\n",
    "print(intens.size())\n",
    "print(output.size())\n",
    "print(intens[0])\n",
    "\n",
    "#Max KE Energy plots\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(logIntens,target[:].cpu().detach().numpy(), color = 'blue')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(duration,target[:].cpu().detach().numpy(), color = 'blue')\n",
    "plt.xlabel('Pulse Duration (fs)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(thickness,target[:].cpu().detach().numpy(), color = 'blue')\n",
    "plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(spotSize,target[:].cpu().detach().numpy(), color = 'blue')\n",
    "plt.xlabel(r'Spot Size  (FWHM, $\\mu m$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the actual data and the regressor's predicted data\n",
    "\n",
    "#Max KE Energy plots\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(logIntens,target[:].cpu().detach().numpy(), color = 'blue')\n",
    "plt.scatter(logIntens,np.exp(output.cpu().detach().numpy()), color = 'red')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(duration,target[:].cpu().detach().numpy(), color = 'blue')\n",
    "plt.scatter(duration,np.exp(output.cpu().detach().numpy()), color = 'red')\n",
    "plt.xlabel('Pulse Duration (fs)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(thickness,target[:].cpu().detach().numpy(), color = 'blue')\n",
    "plt.scatter(thickness,np.exp(output.cpu().detach().numpy()), color = 'red')\n",
    "plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(spotSize,target[:].cpu().detach().numpy(), color = 'blue')\n",
    "plt.scatter(spotSize,np.exp(output.cpu().detach().numpy()), color = 'red')\n",
    "plt.xlabel(r'Spot Size  (FWHM, $\\mu m$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot relative error\n",
    "\n",
    "difference = target[:].cpu().detach().numpy() - np.exp(output.cpu().detach().numpy())\n",
    "error = np.divide(difference, np.exp(output.cpu().detach().numpy())) * 100\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(logIntens, error, color = 'blue')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "plt.ylabel('Max KE Error (%)')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(duration,error, color = 'blue')\n",
    "plt.xlabel('Pulse Duration (fs)')\n",
    "plt.ylabel('Max KE Error (%)')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(thickness,error, color = 'blue')\n",
    "plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "plt.ylabel('Max KE Error (%)')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(spotSize,error, color = 'blue')\n",
    "plt.ylabel('Max KE Error (%)')\n",
    "plt.ylabel('Error (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot relative error magnitude\n",
    "\n",
    "difference = np.abs(target[:].cpu().detach().numpy() - np.exp(output.cpu().detach().numpy()))\n",
    "error = np.divide(difference, np.exp(output.cpu().detach().numpy())) * 100\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(logIntens, error, color = 'blue')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "plt.ylabel('Max KE Error (%)')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(duration,error, color = 'blue')\n",
    "plt.xlabel('Pulse Duration (fs)')\n",
    "plt.ylabel('Max KE Error (%)')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(thickness,error, color = 'blue')\n",
    "plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "plt.ylabel('Max KE Error (%)')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(spotSize,error, color = 'blue')\n",
    "plt.xlabel(r'Spot Size  (FWHM, $\\mu m$)')\n",
    "plt.ylabel('Max KE Error (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also plot absolute errors\n",
    "\n",
    "difference = target[:].cpu().detach().numpy() - np.exp(output.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(logIntens, difference, color = 'blue')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "plt.ylabel('Max KE Error')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(duration, difference, color = 'blue')\n",
    "plt.xlabel('Pulse Duration (fs)')\n",
    "plt.ylabel('Max KE Error')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(thickness, difference, color = 'blue')\n",
    "plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "plt.ylabel('Max KE Error')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(spotSize, difference, color = 'blue')\n",
    "plt.ylabel('Max KE Error')\n",
    "plt.ylabel('Error (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Absolute error magnitude\n",
    "\n",
    "difference = np.abs(target[:].cpu().detach().numpy() - np.exp(output.cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(logIntens, difference, color = 'blue')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "plt.ylabel('Max KE Error')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(duration, difference, color = 'blue')\n",
    "plt.xlabel('Pulse Duration (fs)')\n",
    "plt.ylabel('Max KE Error')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(thickness, difference, color = 'blue')\n",
    "plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "plt.ylabel('Max KE Error')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(spotSize, difference, color = 'blue')\n",
    "plt.ylabel('Max KE Error')\n",
    "plt.ylabel('Error (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Could be better, but we're just using this regression neural network as practice for when we make an invertible regression neural network. There seems to be a small habit of underestimating the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The regresion neural network looks like it does a pretty good job of predicting the max proton energy values, now can we get it to predict all three values at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a new neural network that outputs three features rather than one\n",
    "\n",
    "class MultiRegressor(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.norm0 = nn.BatchNorm1d(4)\n",
    "    self.linear1 = nn.Linear(in_features=4, out_features=64)\n",
    "    self.norm1 = nn.BatchNorm1d(64)\n",
    "    self.act1 = nn.LeakyReLU()\n",
    "    self.dropout = nn.Dropout()\n",
    "    self.linear2 = nn.Linear(in_features=64, out_features=16)\n",
    "    self.norm2 = nn.BatchNorm1d(16)\n",
    "    #self.dropout = nn.Dropout()\n",
    "    self.act2 = nn.LeakyReLU()\n",
    "    self.output = nn.Linear(in_features=16, out_features = 3)\n",
    "    \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    x = self.norm0(x)\n",
    "    x = self.linear1(x)\n",
    "    x = self.norm1(x)\n",
    "    x = self.act1(x)\n",
    "    #x = self.dropout(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.norm2(x)\n",
    "    #x = self.dropout(x)\n",
    "    x = self.act2(x)\n",
    "    x = self.output(x)\n",
    "    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following two cells were used by me to figure out some stuff about the shape of our outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = data[:, 4:7]\n",
    "\n",
    "#print(targets)\n",
    "\n",
    "targets = data[:, 4]\n",
    "print(targets.shape[0])\n",
    "\n",
    "targets = data[:, 4:7]\n",
    "print(targets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = data[:, 4:7]\n",
    "targets = targets.reshape((targets.shape[0], 3))\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our multi-output neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize neural network\n",
    "\n",
    "#model = MultiRegressor()\n",
    "multiModel = MultiRegressor().to('cuda') #GPU Version, comment out if not using GPU\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "#loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(multiModel.parameters(), lr=1e-3)\n",
    "numEpochs = 20\n",
    "\n",
    "print(multiModel)\n",
    "\n",
    "\n",
    "trainNetwork(multiModel, loss_function, optimizer, numEpochs, dataloader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate with test dataset\n",
    "\n",
    "testDataloader = DataLoader(test_dataset, batch_size=5000, shuffle=True)\n",
    "iterDataLoader = iter(testDataloader)\n",
    "testData = next(iterDataLoader)\n",
    "\n",
    "\n",
    "#Old code utilizing the full dataset\n",
    "\n",
    "# fulldata = DataLoader(training_dataset, batch_size=5000, shuffle=True)\n",
    "# iterDataLoader = iter(fulldata)\n",
    "# allData = next(iterDataLoader)\n",
    "\n",
    "# print(allData)\n",
    "\n",
    "#Get each separate input for future plotting reasons\n",
    "intens = testData[:, 0:1]\n",
    "duration = testData[:, 1]\n",
    "thickness = testData[:, 2]\n",
    "spotSize = testData[:, 3]\n",
    "\n",
    "#Transform intens by a log function\n",
    "logIntens = np.log(intens)\n",
    "\n",
    "print(logIntens.size(), testData[:,1:4].size())\n",
    "#Process intensity by putting it on a log scale\n",
    "inputs = torch.cat((logIntens, testData[:,1:4]), axis = 1)\n",
    "\n",
    "#Turn logIntens back into a tensor that can be passed to a plot\n",
    "intens = testData[:, 0]\n",
    "logIntens = np.log(intens)\n",
    "\n",
    "#inputs = allData[:, 0:4]\n",
    "target = testData[:, 4:7]\n",
    "target = np.log(target)\n",
    "\n",
    "inputs = inputs.to('cuda')\n",
    "target = target.to('cuda')\n",
    "\n",
    "inputs, target = inputs.float(), target.float()\n",
    "target = target.reshape((target.shape[0],3))\n",
    "\n",
    "#print(\"Inputs:\", inputs)\n",
    "#print(\"Targets:\", target)\n",
    "\n",
    "output = multiModel(inputs)\n",
    "\n",
    "target = np.log(testData[:, 4:7])\n",
    "\n",
    "print(\"Targets:\", target)\n",
    "#print(output)\n",
    "print(\"Outputs:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target:\", target[:])\n",
    "print(target[:, 0])\n",
    "print(\"Output:\", output)\n",
    "print(output[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It seems the multi-regressor has a habit of under-predicting max and total proton energy by a slight margin. It is way off for average proton energy though, maybe try applying a log-scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot the actual data and the regressor's predicted data\n",
    "\n",
    "#Max KE Energy plots\n",
    "\n",
    "index = 0\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(logIntens,np.exp(target[:, index].cpu().detach().numpy()), color = 'blue')\n",
    "plt.scatter(logIntens,np.exp(output[:, index].cpu().detach().numpy()), color = 'red')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(duration,np.exp(target[:, index].cpu().detach().numpy()), color = 'blue')\n",
    "plt.scatter(duration,np.exp(output[:, index].cpu().detach().numpy()), color = 'red')\n",
    "plt.xlabel('Pulse Duration (fs)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(thickness,np.exp(target[:, index].cpu().detach().numpy()), color = 'blue')\n",
    "plt.scatter(thickness,np.exp(output[:, index].cpu().detach().numpy()), color = 'red')\n",
    "plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(spotSize,np.exp(target[:, index].cpu().detach().numpy()), color = 'blue')\n",
    "plt.scatter(spotSize,np.exp(output[:, index].cpu().detach().numpy()), color = 'red')\n",
    "plt.xlabel(r'Spot Size  (FWHM, $\\mu m$)')\n",
    "plt.ylabel('Max Proton Energy (MeV)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error plots\n",
    "## First max kinetic energy, then total energy, and finally average energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_error(target, output, logIntens, duration, thickness, spotSize, index, label):\n",
    "    difference = np.exp(target[:, index].cpu().detach().numpy()) - np.exp(output[:, index].cpu().detach().numpy())\n",
    "    difference = np.abs(difference)\n",
    "    error = np.divide(difference, np.exp(output[:, index].cpu().detach().numpy())) * 100\n",
    "\n",
    "    fig=plt.figure(figsize=(12,8))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.scatter(logIntens, error, color = 'blue')\n",
    "    plt.xscale('log')\n",
    "    #plt.ylim([0, 100])\n",
    "    plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "    plt.ylabel(label)\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.scatter(duration, error, color = 'blue')\n",
    "    plt.xlabel('Pulse Duration (fs)')\n",
    "    plt.ylabel(label)\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.scatter(thickness,error, color = 'blue')\n",
    "    plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "    plt.ylabel(label)\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.scatter(spotSize, error, color = 'blue')\n",
    "    plt.xlabel(r'Spot Size  (FWHM, $\\mu m$)')\n",
    "    plt.ylabel(label)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive absolute error means the NN had over-estimated the error\n",
    "# Negative absolute error means the NN had under-estimated the error\n",
    "def plot_absolute_error(target, output, logIntens, duration, thickness, spotSize, index, label):\n",
    "    difference = np.exp(output[:, index].cpu().detach().numpy()) - np.exp(target[:, index].cpu().detach().numpy())\n",
    "    absDifference = np.abs(difference)\n",
    "\n",
    "    fig=plt.figure(figsize=(12,8))\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.scatter(logIntens, difference, color = 'blue')\n",
    "    plt.xscale('log')\n",
    "    #plt.ylim([0, -1])\n",
    "    plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "    plt.ylabel(label)\n",
    "\n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.scatter(duration, difference, color = 'blue')\n",
    "    plt.xlabel('Pulse Duration (fs)')\n",
    "    plt.ylabel(label)\n",
    "\n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.scatter(thickness, difference, color = 'blue')\n",
    "    plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "    plt.ylabel(label)\n",
    "\n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.scatter(spotSize, difference, color = 'blue')\n",
    "    plt.xlabel(r'Spot Size  (FWHM, $\\mu m$)')\n",
    "    plt.ylabel(label)\n",
    "\n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.scatter(logIntens, absDifference, color = 'blue')\n",
    "    plt.xscale('log')\n",
    "    #plt.ylim([0, 1])\n",
    "    plt.xlabel(r'Intensity (W cm$^{-2}$)')\n",
    "    plt.ylabel(label + \" (Magnitude)\")\n",
    "\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.scatter(duration, absDifference, color = 'blue')\n",
    "    plt.xlabel('Pulse Duration (fs)')\n",
    "    plt.ylabel(label + \" (Magnitude)\")\n",
    "\n",
    "    plt.subplot(2, 4, 7)\n",
    "    plt.scatter(thickness, absDifference, color = 'blue')\n",
    "    plt.xlabel(r'Target Thickness ($\\mu m$)')\n",
    "    plt.ylabel(label + \" (Magnitude)\")\n",
    "\n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.scatter(spotSize, absDifference, color = 'blue')\n",
    "    plt.xlabel(r'Spot Size  (FWHM, $\\mu m$)')\n",
    "    plt.ylabel(label + \" (Magnitude)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_MSE_Error(target, output, index):\n",
    "    result = np.square(np.subtract(np.exp(target[:, index].cpu().detach().numpy()), np.exp(output[:, index].cpu().detach().numpy())).mean())\n",
    "                       \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Avg_Percent_Error(target, output, index):\n",
    "    difference = np.exp(target[:, index].cpu().detach().numpy()) - np.exp(output[:, index].cpu().detach().numpy())\n",
    "    difference = np.abs(difference)\n",
    "    error = np.divide(difference, np.exp(output[:, index].cpu().detach().numpy())) * 100\n",
    "    \n",
    "    result = error.mean()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot relative error and relative error magnitude\n",
    "\n",
    "plot_relative_error(target, output, logIntens, duration, thickness, spotSize, 0, 'Max KE Error (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also plot absolute errors\n",
    "\n",
    "plot_absolute_error(target, output, logIntens, duration, thickness, spotSize, 0, 'Max KE Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Errors for total proton energy\n",
    "\n",
    "plot_relative_error(target, output, logIntens, duration, thickness, spotSize, 1, 'Total Energy Error (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_absolute_error(target, output, logIntens, duration, thickness, spotSize, 1, 'Total Energy Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, do average energy\n",
    "\n",
    "plot_relative_error(target, output, logIntens, duration, thickness, spotSize, 2, 'Avg Energy Error (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_absolute_error(target, output, logIntens, duration, thickness, spotSize, 2, 'Avg Energy Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean squared error values\n",
    "#Also calculate the average relative error\n",
    "\n",
    "error = [0., 1., 2.]\n",
    "percentError = [0., 1., 2.]\n",
    "\n",
    "for index in range(3):\n",
    "    error[index] = calc_MSE_Error(target, output, index)\n",
    "    percentError[index] = calc_Avg_Percent_Error(target, output, index)\n",
    "    \n",
    "print(\"Max KE MSE:\", error[0])\n",
    "print(\"Max KE Average Relative Error:\", percentError[0])\n",
    "print(\"Total Energy MSE:\", error[1])\n",
    "print(\"Total Energy Average Relative Error:\", percentError[1])\n",
    "print(\"Avg Energy MSE:\", error[2])\n",
    "print(\"Avg Energy Average Relative Error:\", percentError[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#We'll be using matplotlib notebooks so we can easily rotate the 3-D plots interactively\n",
    "#Note that when we have many, many points, these plots can be very slow to respond to rotations\n",
    "\n",
    "size3D = (10,6) #Controls the figure size for our plots\n",
    "\n",
    "fig = plt.figure(figsize=size3D)\n",
    "\n",
    "#Intensity and Pulse Duration\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.mouse_init()\n",
    "ax.set_xlabel('Max Energy', fontweight ='bold')\n",
    "ax.set_ylabel('Total Energy', fontweight ='bold')\n",
    "ax.set_zlabel('Average Energy', fontweight ='bold')\n",
    "ax.scatter3D(target[:, 0].cpu().detach().numpy(), target[:, 1].cpu().detach().numpy(), target[:, 2].cpu().detach().numpy(),\n",
    "            color = 'blue')\n",
    "ax.scatter3D(output[:, 0].cpu().detach().numpy(), output[:, 1].cpu().detach().numpy(), output[:, 2].cpu().detach().numpy(),\n",
    "            color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "- Apply the log function to intensity for neural networks and all of the outputs for the energies\n",
    "- Maybe look into whether or not we only need to apply a log to the average energy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
